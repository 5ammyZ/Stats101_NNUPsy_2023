---
format: 
  revealjs:
    slide-number: true
    logo: images/image-393370223.png
    scrollable: true 
    theme: theme.scss
editor: source
fontsize: 24pt
editor_options: 
  chunk_output_type: inline
---

</br>

<h1 style="text-align: center">

Lecture 12

降维分析</br>---EFA, PCA & Clustering

</h1>

<h2 style="text-align: center">

</h2>

</br>

<h3 style="text-align: center">

Hu Chuan-Peng

</h3>

<h3 style="text-align: center">

2023-11-28

</h3>

</br></br>

# 本次课内容

-   探索性因素分析(Exploratory Factor Analysis)

-   主成分分析(Principal Component Analysis)

-   聚类分析(Cluster Analysis)

## 回顾

-   卡方检验的原理

    -   $\chi^2 = \sum \frac{(f_{observed} - f_{expected})^2}{f_{expected}}$;
    -   $\chi^2$ 反映了实际的观察频数与期望频数的偏离程度。
    -   是离散分布在假设研究中的应用(e.g., Poisson)

-   卡方检验的应用

    -   适合性$\chi^2$检验；
    -   独立性$\chi^2$检验（列联表检验）。

## 卡方检验与广义线性模型（GLM）

-   对数线性模型（log-linear model）

-   Model 1: Freq ~ 1 + Answer_Puppy + Answer_Flower + Agent_Robot

-   Model 2: Freq ~ 1 + Answer_Puppy + Answer_Flower + Agent_Robot + Answer_Puppy * Agent_Robot + Answer_Flower * Agent_Robot</br></br>

![](images/12_1.jpg)

# Part 1: 探索性因素分析(EFA)

## 因素分析（因子分析）</br>

-   以潜在较少数量的未观察变量（因子）用于描述观察到的、相关的变量之间的变异性。

-   用少数因子反映原始资料的大部分信息。

-   观测变量被建模为潜变量(latent variables)和"误差"项的线性组合。

## 

-   因素分析（Factor Analysis）:

    -   探索性因素分析（Exploratory Factor Analysis, EFA）

    -   验证性因素分析（Confirmatory Factor Analysis, CFA）

## 

-   探索性因素分析：探索多变量的内在结构并进行降维的处理技术，将多变量综合为少数几个核心因子。

    -   变量较多，根据相关性大小可将变量分组，组内的变量间相关性较高，不同组的变量间相关性较低。</br></br>

-   验证性因素分析：对因子结构及一系列原变量间的关系是否符合研究者的理论构想进行验证的技术。

## 探索性因素分析

-   探索性因素分析的代数模型：

![](images/12_2.jpg)

## 

-   因素分析的代数模型：

![](images/12_3.jpg)

## 探索性因素分析的代数模型

$$\begin{cases}
 X_1 = \lambda_{11}F_1 + \lambda_{12}F_2+ ... + \lambda_{1m}F_m +\epsilon_1\\
 X_2 = \lambda_{21}F_1 + \lambda_{22}F_2+ ... + \lambda_{2m}F_m +\epsilon_2\\
    \\
 X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2+ ... + \lambda_{im}F_m +\epsilon_i \\
 X_p = \lambda_{p1}F_1 + \lambda_{p2}F_2+ ... + \lambda_{pm}F_m +\epsilon_p
\end{cases}$$

$X_1$, $X_2$, ... $X_p$分别表示某被试在第1个、第2个到第p个变量上的得分，以标准分表示</br> $F_1$，$F_2$, ...$F_m$分别表示某被试在第m个公共因子上的得分，以标准分表示</br> $\lambda_{ij}$表示第i个变量对应回归方程中第j个因子的系数，也称因子载荷

## 因素分析的矩阵形式

-   $X = (X_1, X_2, ... , X_p)$

-   $F = (F_1, F_2, ... , F_m)$

-   $ε = (\epsilon_1, \epsilon_2, ... , \epsilon_m)$ </br> </br>

-   $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + ... + \lambda_{im}F_m + \epsilon_i$

$X_1$, $X_2$, ...$X_p$分别表示某被试在第1个、第2个到第p个变量上的得分，以标准分表示</br> $F_1$，$F_2$, ...$F_m$分别表示某被试在第m个公共因子上的得分，以标准分表示</br> $\lambda_{ij}$表示第i个变量对应回归方程中第j个因子的系数，也称因子载荷

## 变量的共同度

-   $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2 + ... + \lambda_{im}F_m +\epsilon_i$

-   $h_i^2 = \lambda_{i1}^2 + \lambda_{i2}^2 + ... + \lambda_{im}^2$

-   共同度($h_i^2$)在数值上等于因子载荷矩阵中第i行因子载荷的平方和，反映"所有因子对这个变量共同制约的程度"，共同度越接近1越好

## 

-   $X_i = \lambda_{i1}F_1 + \lambda_{i2}F_2+ ... + \lambda_{im}F_m + \epsilon_i$

-   $X_p = \lambda_{p1}F_1 +\lambda_{p2}F_2+ ... + \lambda_{pm}F_m + \epsilon_p$

-   $v_j^2 = {\textstyle \sum_{i = 1}^{p}} \lambda_{ij}^2 = \lambda_{1j}^2 +\lambda_{2j}^2 + ... + \lambda_{pj}^2$

-   反映公共因子对所有变量总的影响，以及其在所有公共因子中的相对重要性

## 因素分析的主要步骤

1.  适合度检验

2.  构造因素模型并确定因子数

3.  因子旋转

4.  因子得分计算

5.  因子命名与解释

## Step 1: 因素分析的适合度检验

-   相关矩阵：大部分相关系数低于0.3且不显著，就不适合因素分析

-   巴特利特球形检验：$H_0$为"相关矩阵是一个单位阵"，根据行列式计算得到统计量，进行卡方检验，结果显著则可进行因素分析

-   反像相关矩阵检验：从偏相关矩阵出发，每个元素都是负的偏相关系数，元素绝对值大，说明受其他变量更迭影响小，不适合因素分析

## Step 1: 因素分析的适合度检验

-   KMO取样适合度检验：结合偏相关矩阵和相关矩阵，比较某变量和其他变量的相关系数与偏相关系数的相对大小。如果相关矩阵中绝对值大，偏相关矩阵元素绝对值小，存在公共因子可能性高，适合因素分析

    -   KMO > 0.9，非常合适；

    -   0.8 < KMO <0.9，合适；

    -   0.7 < KMO < 0.8，一般；

    -   0.6 < KM  O <0.7，不太合适；

    -   KMO < 0.5，极不合适。

## Step 2: 因子提取的方法

-   主成分分析法(principal axis factoring)

-   最小二乘法

-   极大似然法

-   映像分析法

## Step 2: 因子数的确定

-   抽取的m个因子对原变量方差的解释率要达到较高比例，一般建议达到80%，实际运用可根据具体情况调整，也可定在40% ~ 60%。

-   特征值大于1，低于1的特征值表明该因子可解释的变异信息比原变量的变异方差小。

-   通过碎石图确定。选择碎石曲线从迅速下降到平缓的拐点确定因子数。

-   结合定性分析。结合理论与经验分析。

-   Horn's Parallel analysis: 与相同大小的随机矩阵的特征值进行比较

## Step 3: 因子旋转

-   初始因子载荷矩阵的含义往往不明确，如果各列因子载荷值之间差异不明显，很难分化原变量与公共因子的对应关系。

-   因子旋转计算将抽取的因子经过数学变换，分离个因子，凸显其意义，使得每个变量在某因子上有高负荷，在其他因子上负荷较低。

## Step 3: 因子旋转的方法

-   正交旋转：假设各因子间没有相关关系，旋转过程，各因子轴间保持90°夹角。

-   斜交因子旋转：不要求因子轴互相垂直，旋转后每条因子轴更靠近各自变量群，斜交因子轴夹角余弦值是两因子间的相关系数。

    -   难以解释各变量被公共因子解释的比例；

    -   斜交旋转后的每列因子载荷平方和只有偶然情况等于共同度；

    -   斜交旋转后因子可作为变量进行因素分析。

## 

![](images/12_4.jpg)

## Step 4 & 5: 因子分计算与命名

-   因素分析反映的是因子与变量的相关关系，而非因果关系。

-   个体在各因子上得分(factor score)计算：

    -   因子分估计值：

    -   $\widehat{F_j} =W_{j1} × ZX_1 + W_{j2} × ZX_2 + ... + W_{jp} × ZX_p$

        -   $W_{ij}$为标准化后的数据矩阵X的加权系数，反映第i个变量和第j个因子的相关关系
        -   误差项为真因子分F与因子分估计值$\widehat{F}$之间的差异, $E = F - \widehat{F}$

## 示例

大五人格问卷题目分析，表中为每个被试在单个题目的得分

数据：example13.csv (Liu et al., 2020, J Open Psych Data)

```{r echo=TRUE, message=FALSE}
require(pacman)
p_load('tidyverse',
       'bruceR',
       'factoextra', # 多元统计方法的可视化
       'igraph')# 网络数据可视化
```

```{r echo=TRUE}
# 载入数据
df <- read.csv('data/example13.csv')
df
```

```{r echo=TRUE}
bruceR::EFA(df, varrange=c("BFI_A1:BFI_E8"), # 选取观测变量
    nfactors="parallel",             # 决定抽取多少因子
    method="minres",                 # 因子提取方法,最小残差因子分析       
    rotation="oblimin",              # 旋转方法，方差最大化
    kaiser=FALSE,                    # 有没有进行kaiser正态化
    sort.loadings=TRUE,              # 按照因子载荷排序
    hide.loadings=0.45,              # 因子载荷阈值
    plot.scree = TRUE,               # 绘制碎石图
    max.iter = 25,                   # 最大迭代次数
    min.eigen = 1,                   # 最小特征值
    )  
```

## 

![](images/12_5.jpg)

# Part 2: 主成分分析(PCA)

## 

![](images/12_6.jpg)

## 

-   PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法。

-   PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。

## 

![](images/12_7.jpg)

## 标准化

-   PCA对变量的方差较为敏感；

-   标准化可以将变量转化为标准正态分布，以使每个变量的量纲一致，保证PCA的准确性

## 协方差矩阵

-   数据能够进行降维的原因是因为不同变量之间的存在相关，包含冗余信息。

-   通过协方差矩阵，能够"识别"数据间的相关性。

## 

-   协方差矩阵是一个p×p对称矩阵（其中p是维数），它是变量之间的所有相关联的协方差。

-   例如，对于具有 3 个变量x、y和z的三维数据集，协方差矩阵是以下 3×3 矩阵：

-   $\begin{bmatrix} Cov(x,x) & Cov(x,y) & Cov(x,z)\\ Cov(y,x) & Cov(y,y) & Cov(y,z) \\ Cov(z,x) & Cov(z,y) & Cov(z,z) \end{bmatrix}$

## 主成分

-   主成分是变量的线性组合的新变量。

-   这些组合的完成方式是，新变量（即主成分）不相关，并且变量中的大部分信息被压缩或压缩到第一个分量中。

-   10 维数据会得到10个主成分，但PCA试图令第一个主成分包含尽可能多的信息，然后令第二个主成分包含剩余信息中尽可能多的信息，依此类推，直到得到下面碎石图所示的东西。

## 

![](images/12_8.jpg)

## 

-   主成分的可解释性较差，可能没有任何实际意义（它们仅是初始变量的线性组合）。

-   从几何上讲，主成分表示解释最大方差量的数据的方向，即捕获数据大部分信息的线。

-   一条线携带的方差越大，沿它的数据点的离散度就越大，沿一条线的方差越大，它拥有的信息就越多。

-   主成分可以被当作新的空间轴，帮助我们更好地看到观测值间的差异。

## 如何构造主成分

-   主成分的数量可与变量一样多；

-   第一个主成分是能够代表数据中最大可能的方差的成分

-   第二个主成分与第一个主成分不相关（即垂直于），能解释剩下方差中最大的比例。

-   不断地计算，直到得到p 个主成分。

## 沿主成分轴重铸数据

-   具体而言，在计算主成分过程中，使用协方差矩阵的特征向量形成的特征向量，将数据从原始轴重新定向到主分量表示的轴（因此称为主分量分析）。

-   通过将原始数据集的转置乘以特征向量的转置来完成。

## PCA的本质

-   PCA本质上是求出协方差矩阵的特征值和特征向量。

-   目前对协方差矩阵的特征值分解现在是基于奇异值分解进行。

-   PCA对主成分进行选择其实就是选择特征值较大的几个特征向量。

## 

-   数据降维度会导致信息的损失，所以降维的代价是牺牲精确性。

-   PCA降维：舍弃方差较小的主成分，保留方差较大的主成分。

## 

![](images/12_9.jpg)

-   紫色的线是最大化方差（从投影点（红点）到原点的平方距离的平均值）的线。

-   https://builtin.com/data-science/step-step-explanation-principal-component-analysis

## 演示

-   https://setosa.io/ev/principal-component-analysis/

## 应用

-   PCA可以用来进行图像压缩，通过舍弃信息较小的维度，保留信息较大的维度，从而实现降低图片的大小。

![](images/12_10.jpg)

https://blog.csdn.net/ctyqy2015301200079/article/details/85325125

## 

在机器学习中，还有k-Neareast Neighbours(K-近邻)，随机森林等一系列降维算法。

## 示例

```{r echo=TRUE}
bruceR::EFA(df, varrange=c("BFI_A1:BFI_E8"), # 选取观测变量
    nfactors="parallel",                     # 决定抽取多少因子
    method="pca",                            # 主成分分析       
    rotation="none",                         # 旋转方法，none
    kaiser=FALSE,                            # 有没有进行kaiser正态化
    sort.loadings=TRUE,                      # 按照因子载荷排序
    hide.loadings=0.45,                      # 因子载荷阈值
    plot.scree = TRUE,                       # 绘制碎石图
    max.iter = 25,                           # 最大迭代次数
    min.eigen = 1,                           # 最小特征值
    )  
```

## 

![](images/12_11.jpg)

## 

![](images/12_12.jpg)

# Part 3: 聚类分析

## 聚类分析

-   将分类对象置于多维空间，按亲疏远近分类。

-   以事物间的定量差异为数学基础，定性差异为结果选择的依据。

-   聚类的目标是得到较高的簇内相似度和较低的簇间相似度，使得簇间的距离尽可能大，簇内样本与簇中心的距离尽可能小。

## 

-   聚类得到的簇可以用聚类中心、簇大小、簇密度等来表示。

    -   聚类中心是一个簇中所有样本点的均值(质心)

    -   簇大小表示簇中所含样本的数量

    -   簇密度表示簇中样本点的紧密程度

## 聚类分析的依据

-   用于对聚类结果进行评判，分为内部指标和外部指标两大类：

    -   内部指标是指不借助任何外部参考，只用参与聚类的样本评判聚类结果好坏

    -   外部指标指用事先指定的聚类模型作为参考来评判聚类结果的好坏

## 聚类分析的度量

-   内部指标

    -   在聚类分析中，对于两个m维样本$x_i = (x_{i1}, x_{i2}, ... , x_{im})$ 和 $x_j = (x_{j1}, x_{j2}, ... , x_{jm})$。

    -   距离的测量包括距离和相似性系数两种类型。

    -   常用的距离度量有欧式距离、曼哈顿距离、切比雪夫距离和明可夫斯基距离等。

## 

-   欧式距离（Euclidean Distance）是计算欧式空间中两点之间的直线距离

-   $dict_{ED} = \sqrt{{\textstyle \sum_{k = 1}^{m}} (x_{ik} - x_{jk})^2}$

## 

-   曼哈顿距离（ManhattanDistance）也称绝对值距离，类似于在城市中两个地点之间的实际距离，即沿道路行驶的距离。

-   计算空间中两点各维度指标间差值的绝对值之和。

-   $dict_{mand} ={\textstyle \sum_{k = 1}^{m}} \left | (x_{ik} - x_{jk})^2 \right |$

## 

-   切比雪夫距离（Chebyshev Distance），向量空间中的距离度量。

-   将空间坐标中两个点的距离定义为其各坐标数值差绝对值的最大值。

-   $dict_{CD} =max \left | (x_{ik} - x_{jk}) \right |$

## 测量指标的量纲统一

-   数据中心化变换

-   数据标准化变换

-   极差正规化变换

-   对数变换

## 聚类的分类

-   基于层次的聚类

-   基于划分的聚类(k‐mean算法、k‐medoids算法、k‐prototype算法)

-   基于密度的聚类(DBSCAN算法、OPTICS算法、DENCLUE算法)

-   基于网格的聚类

-   基于模型的聚类(模糊聚类、Kohonen神经网络聚类)

## 基于层次的聚类

-   层次聚类分析就是逐次聚类分析，将观测样本的每一个个体或每一个变量看作小类，计算两两间距离，比较距离，距离最小的归为一类，不断进行此过程。

-   对个案的层次聚类分析叫Q聚类分析，对变量的层次聚类分析叫R聚类分析。

## 

![](images/12_13.jpg)

https://zhuanlan.zhihu.com/p/37856153

## 

![](images/12_14.jpg)

https://zhuanlan.zhihu.com/p/37856153

## 层次聚类分析

-   步骤：

    -   数据获取

    -   距离计算与逐步凝聚

    -   绘制凝聚状态表，树状图，冰柱图

    -   确定类别数和个体的类属关系

## 距离计算与逐步凝聚

-   个案距离计算：

    -   连续变量：欧氏距离，切比雪夫距离，绝对值距离

    -   顺序，等级变量：卡方分析

    -   二分变量：二元欧氏距离平方

## 距离计算与逐步凝聚

-   个案与小类，小类与小类距离计算：

    -   最短距离法

    -   最长距离法

    -   类间平均连锁法

    -   重心法

## 示例

```{r echo=TRUE}
df <- read.csv('data/example13_2.csv')
rownames(df) <- df$X # 将第一列命名为索引
df <- df[,-1]        # 去除原本的第一列
df <- scale(df)      # 将数据进行正态化
```

```{r echo=TRUE}
d <- dist(df)          # 计算欧氏距离
d
```

```{r echo=TRUE}
fit1 <- hclust(d,method = "average") # 层次聚类分析

plot(fit1,hang = -1,cex=.8,main = "title") # 树状图
```

```{r echo=TRUE}
fviz_dend(fit1, k=4, rect =T, rect_fill = T, type = "circular",
          rect_border = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07")) # 圆形视图
```

```{r echo=TRUE}
heatmap(as.matrix(d)) # 热力图
```

## 总结

|            |                      |                        |                  |
|:----------------:|:----------------:|:-----------------:|:----------------:|
|            |         EFA          |          PCA           |     Clusters     |
|    原理    | 变量是因子的线性组合 | 主成分是变量的线性组合 | 相似变量归为一类 |
|    应用    |   Reflective model   |    Formative model     |       分类       |
|   新变量   |     独立 / 相关      |          独立          |        /         |
| 数据预处理 |          /           |         标准化         |        /         |
